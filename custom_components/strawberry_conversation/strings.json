{
  "config": {
    "step": {
      "user": {
        "title": "Connect to Strawberry Hub",
        "description": "Enter your Strawberry Hub URL and device token to connect.",
        "data": {
          "hub_url": "Hub URL",
          "hub_token": "Device Token"
        },
        "data_description": {
          "hub_url": "The URL of your Strawberry Hub (e.g., http://192.168.1.100:8000)",
          "hub_token": "JWT device token from Hub device registration"
        }
      }
    },
    "error": {
      "cannot_connect": "Unable to connect to the Strawberry Hub. Check the URL and ensure the Hub is running.",
      "invalid_auth": "Authentication failed. Check your device token.",
      "unknown": "An unexpected error occurred."
    },
    "abort": {
      "already_configured": "This Hub is already configured."
    }
  },
  "config_subentries": {
    "conversation": {
      "initiate_flow": {
        "user": "Add conversation agent"
      },
      "step": {
        "set_options": {
          "title": "Conversation Options",
          "data": {
            "name": "Name",
            "prompt": "System Prompt",
            "llm_hass_api": "Control Home Assistant",
            "advanced_fallback": "Show advanced offline fallback settings",
            "routing_section": "General Offline Routing",
            "offline_backend": "Offline backend",
            "tensorzero_function_name": "TensorZero function",
            "offline_provider": "Offline LLM Provider",
            "offline_fallback_providers": "Offline fallback provider order",
            "openai_section": "OpenAI Configuration",
            "offline_openai_api_key": "OpenAI API Key",
            "offline_openai_model": "OpenAI Model",
            "google_section": "Google Configuration",
            "offline_google_api_key": "Google API Key",
            "offline_google_model": "Google Model",
            "anthropic_section": "Anthropic Configuration",
            "offline_anthropic_api_key": "Anthropic API Key",
            "offline_anthropic_model": "Anthropic Model",
            "ollama_section": "Ollama Configuration",
            "offline_ollama_model": "Ollama Model",
            "ollama_url": "Ollama URL"
          },
          "data_description": {
            "prompt": "Custom instructions for the AI. Supports Home Assistant templates.",
            "llm_hass_api": "Allow the AI to control exposed Home Assistant entities.",
            "advanced_fallback": "Toggle to reveal advanced configuration for offline LLM providers and fallback routing.",
            "offline_backend": "Backend used for offline mode: Auto prefers TensorZero when installed; OpenAI-compatible uses direct provider APIs.",
            "tensorzero_function_name": "The specific function name defined in your TensorZero configuration (e.g., 'chat' or 'ha_chat').",
            "offline_provider": "Primary LLM provider to use when the Hub is unreachable.",
            "offline_fallback_providers": "Ordered fallback providers to try if the primary provider fails.",
            "offline_openai_api_key": "API key used when OpenAI is selected in the offline chain.",
            "offline_google_model": "Google model for offline mode.",
            "offline_anthropic_api_key": "API key used when Anthropic is selected in the offline chain.",
            "offline_anthropic_model": "Anthropic model for offline mode.",
            "offline_ollama_model": "Ollama model for offline mode.",
            "ollama_url": "URL for local Ollama OpenAI-compatible endpoint."
          }
        }
      }
    }
  },
  "entity": {
    "conversation": {
      "strawberry": {
        "name": "Strawberry"
      }
    }
  }
}
